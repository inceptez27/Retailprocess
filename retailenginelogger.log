======process started at 2022-04-18 8:30:59
Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.204.147 instead (on interface ens33)
Set SPARK_LOCAL_IP if you need to bind to another address
Running Spark version 2.4.7
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Submitted application: Retail-coreengine
Changing view acls to: hduser
Changing modify acls to: hduser
Changing view acls groups to: 
Changing modify acls groups to: 
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hduser); groups with view permissions: Set(); users  with modify permissions: Set(hduser); groups with modify permissions: Set()
Successfully started service 'sparkDriver' on port 36150.
Registering MapOutputTracker
Registering BlockManagerMaster
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
BlockManagerMasterEndpoint up
Created local directory at /tmp/blockmgr-940e88ba-e4df-40a1-a005-b6e0999f75f6
MemoryStore started with capacity 1202.4 MB
Registering OutputCommitCoordinator
Logging initialized @2246ms
jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
Started @2330ms
Started ServerConnector@613a8ee1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
Successfully started service 'SparkUI' on port 4040.
Started o.s.j.s.ServletContextHandler@64ba3208{/jobs,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@13518f37{/jobs/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@7cbc3762{/jobs/job,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@49872d67{/jobs/job/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@56303b57{/stages,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4b2a01d4{/stages/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@8692d67{/stages/stage,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@34f7234e{/stages/stage/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@753432a2{/stages/pool,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@23bff419{/stages/pool/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4983159f{/storage,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@44e3a2b2{/storage/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@101639ae{/storage/rdd,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4c550889{/storage/rdd/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@1d2bd371{/environment,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@44040454{/environment/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@65fe9e33{/executors,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@18bc345{/executors/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@42f8285e{/executors/threadDump,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@26bab2f1{/executors/threadDump/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@3724af13{/static,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@293a5f75{/,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@fcb4004{/api,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@784c3487{/jobs/job/kill,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@53142455{/stages/stage/kill,null,AVAILABLE,@Spark}
Bound SparkUI to 0.0.0.0, and started at http://192.168.204.147:4040
Starting executor ID driver on host localhost
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41003.
Server created on 192.168.204.147:41003
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
Registering BlockManager BlockManagerId(driver, 192.168.204.147, 41003, None)
Registering block manager 192.168.204.147:41003 with 1202.4 MB RAM, BlockManagerId(driver, 192.168.204.147, 41003, None)
Registered BlockManager BlockManagerId(driver, 192.168.204.147, 41003, None)
Initialized BlockManager: BlockManagerId(driver, 192.168.204.147, 41003, None)
Sink class org.apache.spark.metrics.sink.MetricsServlet cannot be instantiated
Error initializing SparkContext.
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply(MetricsSystem.scala:202)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply(MetricsSystem.scala:196)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at org.apache.spark.metrics.MetricsSystem.registerSinks(MetricsSystem.scala:196)
	at org.apache.spark.metrics.MetricsSystem.start(MetricsSystem.scala:104)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:514)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)
	at retail.driver.rundriver$.main(rundriver.scala:42)
	at retail.driver.rundriver.main(rundriver.scala)
Caused by: java.lang.NoClassDefFoundError: com/fasterxml/jackson/annotation/ObjectIdResolver
	at com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:543)
	at com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:448)
	at org.apache.spark.metrics.sink.MetricsServlet.<init>(MetricsServlet.scala:48)
	... 21 more
Caused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.annotation.ObjectIdResolver
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 24 more
Stopped Spark@613a8ee1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
Stopped Spark web UI at http://192.168.204.147:4040
MapOutputTrackerMasterEndpoint stopped!
MemoryStore cleared
BlockManager stopped
BlockManagerMaster stopped
OutputCommitCoordinator stopped!
Successfully stopped SparkContext
Error Occured:null
Shutdown hook called
Deleting directory /tmp/spark-8dfe461f-6008-4a0d-ad5c-f8dc9c88c529
======process started at 2022-04-18 8:31:11
Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.204.147 instead (on interface ens33)
Set SPARK_LOCAL_IP if you need to bind to another address
Running Spark version 2.4.7
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Submitted application: Retail-coreengine
Changing view acls to: hduser
Changing modify acls to: hduser
Changing view acls groups to: 
Changing modify acls groups to: 
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hduser); groups with view permissions: Set(); users  with modify permissions: Set(hduser); groups with modify permissions: Set()
Successfully started service 'sparkDriver' on port 39391.
Registering MapOutputTracker
Registering BlockManagerMaster
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
BlockManagerMasterEndpoint up
Created local directory at /tmp/blockmgr-8f4e41ce-3d7d-48e8-99a1-27379a46db14
MemoryStore started with capacity 1202.4 MB
Registering OutputCommitCoordinator
Logging initialized @2140ms
jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
Started @2221ms
Started ServerConnector@3edd6c9c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
Successfully started service 'SparkUI' on port 4040.
Started o.s.j.s.ServletContextHandler@23fb172e{/jobs,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@3a393455{/jobs/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@13518f37{/jobs/job,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@3a6f2de3{/jobs/job/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@49872d67{/stages,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@56303b57{/stages/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4b2a01d4{/stages/stage,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@2f48b3d2{/stages/stage/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@34f7234e{/stages/pool,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@753432a2{/stages/pool/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@23bff419{/storage,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4983159f{/storage/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@44e3a2b2{/storage/rdd,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@101639ae{/storage/rdd/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4c550889{/environment,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@1d2bd371{/environment/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@44040454{/executors,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@65fe9e33{/executors/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@18bc345{/executors/threadDump,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@42f8285e{/executors/threadDump/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@26bab2f1{/static,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@60957c0f{/,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@293a5f75{/api,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@2dd29a59{/jobs/job/kill,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@784c3487{/stages/stage/kill,null,AVAILABLE,@Spark}
Bound SparkUI to 0.0.0.0, and started at http://192.168.204.147:4040
Starting executor ID driver on host localhost
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45605.
Server created on 192.168.204.147:45605
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
Registering BlockManager BlockManagerId(driver, 192.168.204.147, 45605, None)
Registering block manager 192.168.204.147:45605 with 1202.4 MB RAM, BlockManagerId(driver, 192.168.204.147, 45605, None)
Registered BlockManager BlockManagerId(driver, 192.168.204.147, 45605, None)
Initialized BlockManager: BlockManagerId(driver, 192.168.204.147, 45605, None)
Sink class org.apache.spark.metrics.sink.MetricsServlet cannot be instantiated
Error initializing SparkContext.
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply(MetricsSystem.scala:202)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply(MetricsSystem.scala:196)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at org.apache.spark.metrics.MetricsSystem.registerSinks(MetricsSystem.scala:196)
	at org.apache.spark.metrics.MetricsSystem.start(MetricsSystem.scala:104)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:514)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)
	at retail.driver.rundriver$.main(rundriver.scala:42)
	at retail.driver.rundriver.main(rundriver.scala)
Caused by: java.lang.NoClassDefFoundError: com/fasterxml/jackson/annotation/ObjectIdResolver
	at com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:543)
	at com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:448)
	at org.apache.spark.metrics.sink.MetricsServlet.<init>(MetricsServlet.scala:48)
	... 21 more
Caused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.annotation.ObjectIdResolver
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 24 more
Stopped Spark@3edd6c9c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
Stopped Spark web UI at http://192.168.204.147:4040
MapOutputTrackerMasterEndpoint stopped!
MemoryStore cleared
BlockManager stopped
BlockManagerMaster stopped
OutputCommitCoordinator stopped!
Successfully stopped SparkContext
Error Occured:null
Shutdown hook called
Deleting directory /tmp/spark-41cf2f1f-0fb8-4050-8de5-af8a6654617b
======process started at 2022-04-18 7:24:38
Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.204.147 instead (on interface ens33)
Set SPARK_LOCAL_IP if you need to bind to another address
Running Spark version 2.4.7
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Submitted application: Retail-coreengine
Changing view acls to: hduser
Changing modify acls to: hduser
Changing view acls groups to: 
Changing modify acls groups to: 
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hduser); groups with view permissions: Set(); users  with modify permissions: Set(hduser); groups with modify permissions: Set()
Successfully started service 'sparkDriver' on port 34279.
Registering MapOutputTracker
Registering BlockManagerMaster
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
BlockManagerMasterEndpoint up
Created local directory at /tmp/blockmgr-e60f9b67-3225-4df6-abbb-704b7c17447a
MemoryStore started with capacity 1202.4 MB
Registering OutputCommitCoordinator
Logging initialized @23610ms
jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
Started @24060ms
Started ServerConnector@613a8ee1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
Successfully started service 'SparkUI' on port 4040.
Started o.s.j.s.ServletContextHandler@64ba3208{/jobs,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@13518f37{/jobs/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@7cbc3762{/jobs/job,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@49872d67{/jobs/job/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@56303b57{/stages,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4b2a01d4{/stages/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@8692d67{/stages/stage,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@34f7234e{/stages/stage/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@753432a2{/stages/pool,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@23bff419{/stages/pool/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4983159f{/storage,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@44e3a2b2{/storage/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@101639ae{/storage/rdd,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4c550889{/storage/rdd/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@1d2bd371{/environment,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@44040454{/environment/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@65fe9e33{/executors,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@18bc345{/executors/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@42f8285e{/executors/threadDump,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@26bab2f1{/executors/threadDump/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@3724af13{/static,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@293a5f75{/,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@fcb4004{/api,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@784c3487{/jobs/job/kill,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@53142455{/stages/stage/kill,null,AVAILABLE,@Spark}
Bound SparkUI to 0.0.0.0, and started at http://192.168.204.147:4040
Starting executor ID driver on host localhost
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46808.
Server created on 192.168.204.147:46808
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
Registering BlockManager BlockManagerId(driver, 192.168.204.147, 46808, None)
Registering block manager 192.168.204.147:46808 with 1202.4 MB RAM, BlockManagerId(driver, 192.168.204.147, 46808, None)
Registered BlockManager BlockManagerId(driver, 192.168.204.147, 46808, None)
Initialized BlockManager: BlockManagerId(driver, 192.168.204.147, 46808, None)
Sink class org.apache.spark.metrics.sink.MetricsServlet cannot be instantiated
Error initializing SparkContext.
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply(MetricsSystem.scala:202)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply(MetricsSystem.scala:196)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at org.apache.spark.metrics.MetricsSystem.registerSinks(MetricsSystem.scala:196)
	at org.apache.spark.metrics.MetricsSystem.start(MetricsSystem.scala:104)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:514)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)
	at retail.driver.rundriver$.main(rundriver.scala:42)
	at retail.driver.rundriver.main(rundriver.scala)
Caused by: java.lang.NoClassDefFoundError: com/fasterxml/jackson/annotation/ObjectIdResolver
	at com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:543)
	at com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:448)
	at org.apache.spark.metrics.sink.MetricsServlet.<init>(MetricsServlet.scala:48)
	... 21 more
Caused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.annotation.ObjectIdResolver
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 24 more
Stopped Spark@613a8ee1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
Stopped Spark web UI at http://192.168.204.147:4040
MapOutputTrackerMasterEndpoint stopped!
MemoryStore cleared
BlockManager stopped
BlockManagerMaster stopped
OutputCommitCoordinator stopped!
Successfully stopped SparkContext
Error Occured:null
Shutdown hook called
Deleting directory /tmp/spark-ac61bd23-8387-4d9b-8240-6aab23ba57b0
======process started at 2022-04-18 7:27:2
Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.204.147 instead (on interface ens33)
Set SPARK_LOCAL_IP if you need to bind to another address
Running Spark version 2.4.7
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Submitted application: Retail-coreengine
Changing view acls to: hduser
Changing modify acls to: hduser
Changing view acls groups to: 
Changing modify acls groups to: 
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hduser); groups with view permissions: Set(); users  with modify permissions: Set(hduser); groups with modify permissions: Set()
Successfully started service 'sparkDriver' on port 33811.
Registering MapOutputTracker
Registering BlockManagerMaster
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
BlockManagerMasterEndpoint up
Created local directory at /tmp/blockmgr-68fe5fe0-3417-414b-bf13-ae6a5bc9ef4b
MemoryStore started with capacity 1202.4 MB
Registering OutputCommitCoordinator
Logging initialized @3540ms
jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
Started @4399ms
Started ServerConnector@613a8ee1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
Successfully started service 'SparkUI' on port 4040.
Started o.s.j.s.ServletContextHandler@64ba3208{/jobs,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@13518f37{/jobs/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@7cbc3762{/jobs/job,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@49872d67{/jobs/job/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@56303b57{/stages,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4b2a01d4{/stages/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@8692d67{/stages/stage,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@34f7234e{/stages/stage/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@753432a2{/stages/pool,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@23bff419{/stages/pool/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4983159f{/storage,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@44e3a2b2{/storage/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@101639ae{/storage/rdd,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4c550889{/storage/rdd/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@1d2bd371{/environment,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@44040454{/environment/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@65fe9e33{/executors,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@18bc345{/executors/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@42f8285e{/executors/threadDump,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@26bab2f1{/executors/threadDump/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@3724af13{/static,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@293a5f75{/,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@fcb4004{/api,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@784c3487{/jobs/job/kill,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@53142455{/stages/stage/kill,null,AVAILABLE,@Spark}
Bound SparkUI to 0.0.0.0, and started at http://192.168.204.147:4040
Starting executor ID driver on host localhost
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36745.
Server created on 192.168.204.147:36745
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
Registering BlockManager BlockManagerId(driver, 192.168.204.147, 36745, None)
Registering block manager 192.168.204.147:36745 with 1202.4 MB RAM, BlockManagerId(driver, 192.168.204.147, 36745, None)
Registered BlockManager BlockManagerId(driver, 192.168.204.147, 36745, None)
Initialized BlockManager: BlockManagerId(driver, 192.168.204.147, 36745, None)
Sink class org.apache.spark.metrics.sink.MetricsServlet cannot be instantiated
Error initializing SparkContext.
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply(MetricsSystem.scala:202)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply(MetricsSystem.scala:196)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)
	at org.apache.spark.metrics.MetricsSystem.registerSinks(MetricsSystem.scala:196)
	at org.apache.spark.metrics.MetricsSystem.start(MetricsSystem.scala:104)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:514)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)
	at retail.driver.rundriver$.main(rundriver.scala:42)
	at retail.driver.rundriver.main(rundriver.scala)
Caused by: java.lang.NoClassDefFoundError: com/fasterxml/jackson/annotation/ObjectIdResolver
	at com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:543)
	at com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:448)
	at org.apache.spark.metrics.sink.MetricsServlet.<init>(MetricsServlet.scala:48)
	... 21 more
Caused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.annotation.ObjectIdResolver
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 24 more
Stopped Spark@613a8ee1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
Stopped Spark web UI at http://192.168.204.147:4040
MapOutputTrackerMasterEndpoint stopped!
MemoryStore cleared
BlockManager stopped
BlockManagerMaster stopped
OutputCommitCoordinator stopped!
Successfully stopped SparkContext
Error Occured:null
Shutdown hook called
Deleting directory /tmp/spark-690d202f-d6c2-4deb-83b7-2ab385462944
======process started at 2022-04-18 7:31:53
Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.204.147 instead (on interface ens33)
Set SPARK_LOCAL_IP if you need to bind to another address
Running Spark version 2.4.7
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Submitted application: Retail-coreengine
Changing view acls to: hduser
Changing modify acls to: hduser
Changing view acls groups to: 
Changing modify acls groups to: 
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hduser); groups with view permissions: Set(); users  with modify permissions: Set(hduser); groups with modify permissions: Set()
Successfully started service 'sparkDriver' on port 34257.
Registering MapOutputTracker
Registering BlockManagerMaster
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
BlockManagerMasterEndpoint up
Created local directory at /tmp/blockmgr-868d0f04-85a2-49cd-82ac-1c937f741dfe
MemoryStore started with capacity 1202.4 MB
Registering OutputCommitCoordinator
Logging initialized @3375ms
jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
Started @3447ms
Started ServerConnector@613a8ee1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
Successfully started service 'SparkUI' on port 4040.
Started o.s.j.s.ServletContextHandler@64ba3208{/jobs,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@13518f37{/jobs/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@7cbc3762{/jobs/job,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@49872d67{/jobs/job/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@56303b57{/stages,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4b2a01d4{/stages/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@8692d67{/stages/stage,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@34f7234e{/stages/stage/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@753432a2{/stages/pool,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@23bff419{/stages/pool/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4983159f{/storage,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@44e3a2b2{/storage/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@101639ae{/storage/rdd,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4c550889{/storage/rdd/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@1d2bd371{/environment,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@44040454{/environment/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@65fe9e33{/executors,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@18bc345{/executors/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@42f8285e{/executors/threadDump,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@26bab2f1{/executors/threadDump/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@3724af13{/static,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@293a5f75{/,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@fcb4004{/api,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@784c3487{/jobs/job/kill,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@53142455{/stages/stage/kill,null,AVAILABLE,@Spark}
Bound SparkUI to 0.0.0.0, and started at http://192.168.204.147:4040
Starting executor ID driver on host localhost
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42040.
Server created on 192.168.204.147:42040
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
Registering BlockManager BlockManagerId(driver, 192.168.204.147, 42040, None)
Registering block manager 192.168.204.147:42040 with 1202.4 MB RAM, BlockManagerId(driver, 192.168.204.147, 42040, None)
Registered BlockManager BlockManagerId(driver, 192.168.204.147, 42040, None)
Initialized BlockManager: BlockManagerId(driver, 192.168.204.147, 42040, None)
Started o.s.j.s.ServletContextHandler@64712be{/metrics/json,null,AVAILABLE,@Spark}
======staging process started at 2022-04-18 7:32:28
reading data from the file:s3a://inceptezspark/RetailData/Retail_Customers.csv
written data into hive table:retail_stg.tblcustomer_stg
reading data from the file:s3a://inceptezspark/RetailData/Retail_Product_Categories.csv
written data into hive table:retail_stg.tblproductcategory_stg
reading data from the file:s3a://inceptezspark/RetailData/Retail_Product_Subcategories.csv
written data into hive table:retail_stg.tblproductsubcategory_stg
reading data from the file:s3a://inceptezspark/RetailData/Retail_Sales*
The directory s3a://inceptezspark/RetailData/Retail_Sales* was not found. Was it deleted very recently?
written data into hive table:retail_stg.tblsales_stg
reading data from the file:s3a://inceptezspark/RetailData/Retail_Territories.csv
written data into hive table:retail_stg.tblterritory_stg
reading data from the file:s3a://inceptezspark/RetailData/Retail_Products.csv
written data into hive table:retail_stg.tblproduct_stg
======staging process completed at 2022-04-18 7:34:45
======process started at 2022-04-18 7:43:19
Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.204.147 instead (on interface ens33)
Set SPARK_LOCAL_IP if you need to bind to another address
Running Spark version 2.4.7
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Submitted application: Retail-coreengine
Changing view acls to: hduser
Changing modify acls to: hduser
Changing view acls groups to: 
Changing modify acls groups to: 
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hduser); groups with view permissions: Set(); users  with modify permissions: Set(hduser); groups with modify permissions: Set()
Successfully started service 'sparkDriver' on port 36782.
Registering MapOutputTracker
Registering BlockManagerMaster
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
BlockManagerMasterEndpoint up
Created local directory at /tmp/blockmgr-2ca89be4-df25-4dab-9691-683f3fc0017f
MemoryStore started with capacity 1202.4 MB
Registering OutputCommitCoordinator
Logging initialized @2088ms
jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
Started @2172ms
Started ServerConnector@613a8ee1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
Successfully started service 'SparkUI' on port 4040.
Started o.s.j.s.ServletContextHandler@64ba3208{/jobs,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@13518f37{/jobs/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@7cbc3762{/jobs/job,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@49872d67{/jobs/job/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@56303b57{/stages,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4b2a01d4{/stages/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@8692d67{/stages/stage,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@34f7234e{/stages/stage/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@753432a2{/stages/pool,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@23bff419{/stages/pool/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4983159f{/storage,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@44e3a2b2{/storage/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@101639ae{/storage/rdd,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4c550889{/storage/rdd/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@1d2bd371{/environment,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@44040454{/environment/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@65fe9e33{/executors,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@18bc345{/executors/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@42f8285e{/executors/threadDump,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@26bab2f1{/executors/threadDump/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@3724af13{/static,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@293a5f75{/,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@fcb4004{/api,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@784c3487{/jobs/job/kill,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@53142455{/stages/stage/kill,null,AVAILABLE,@Spark}
Bound SparkUI to 0.0.0.0, and started at http://192.168.204.147:4040
Starting executor ID driver on host localhost
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46294.
Server created on 192.168.204.147:46294
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
Registering BlockManager BlockManagerId(driver, 192.168.204.147, 46294, None)
Registering block manager 192.168.204.147:46294 with 1202.4 MB RAM, BlockManagerId(driver, 192.168.204.147, 46294, None)
Registered BlockManager BlockManagerId(driver, 192.168.204.147, 46294, None)
Initialized BlockManager: BlockManagerId(driver, 192.168.204.147, 46294, None)
Started o.s.j.s.ServletContextHandler@64712be{/metrics/json,null,AVAILABLE,@Spark}
======staging process started at 2022-04-18 7:43:29
reading data from the file:s3a://inceptezspark/RetailData/Retail_Customers.csv
======process started at 2022-04-19 7:44:55
Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.204.147 instead (on interface ens33)
Set SPARK_LOCAL_IP if you need to bind to another address
Running Spark version 2.4.7
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Submitted application: Retail-coreengine
Changing view acls to: hduser
Changing modify acls to: hduser
Changing view acls groups to: 
Changing modify acls groups to: 
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hduser); groups with view permissions: Set(); users  with modify permissions: Set(hduser); groups with modify permissions: Set()
Successfully started service 'sparkDriver' on port 42448.
Registering MapOutputTracker
Registering BlockManagerMaster
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
BlockManagerMasterEndpoint up
Created local directory at /tmp/blockmgr-3c15d90d-439a-42db-9542-7f0bc56e87d9
MemoryStore started with capacity 1202.4 MB
Registering OutputCommitCoordinator
Logging initialized @4145ms
jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
Started @4485ms
Started ServerConnector@613a8ee1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
Successfully started service 'SparkUI' on port 4040.
Started o.s.j.s.ServletContextHandler@64ba3208{/jobs,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@13518f37{/jobs/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@7cbc3762{/jobs/job,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@49872d67{/jobs/job/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@56303b57{/stages,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4b2a01d4{/stages/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@8692d67{/stages/stage,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@34f7234e{/stages/stage/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@753432a2{/stages/pool,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@23bff419{/stages/pool/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4983159f{/storage,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@44e3a2b2{/storage/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@101639ae{/storage/rdd,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4c550889{/storage/rdd/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@1d2bd371{/environment,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@44040454{/environment/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@65fe9e33{/executors,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@18bc345{/executors/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@42f8285e{/executors/threadDump,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@26bab2f1{/executors/threadDump/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@3724af13{/static,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@293a5f75{/,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@fcb4004{/api,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@784c3487{/jobs/job/kill,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@53142455{/stages/stage/kill,null,AVAILABLE,@Spark}
Bound SparkUI to 0.0.0.0, and started at http://192.168.204.147:4040
Starting executor ID driver on host localhost
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39578.
Server created on 192.168.204.147:39578
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
Registering BlockManager BlockManagerId(driver, 192.168.204.147, 39578, None)
Registering block manager 192.168.204.147:39578 with 1202.4 MB RAM, BlockManagerId(driver, 192.168.204.147, 39578, None)
Registered BlockManager BlockManagerId(driver, 192.168.204.147, 39578, None)
Initialized BlockManager: BlockManagerId(driver, 192.168.204.147, 39578, None)
Started o.s.j.s.ServletContextHandler@30ed9c6c{/metrics/json,null,AVAILABLE,@Spark}
Failed to connect to the MetaStore Server...
Failed to connect to the MetaStore Server...
Failed to connect to the MetaStore Server...
Failed to access metastore. This class should not accessed in runtime.
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1236)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:185)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:118)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:271)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:384)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:286)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:215)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:214)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:116)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:104)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.org$apache$spark$sql$hive$HiveSessionStateBuilder$$externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$1.apply(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$1.apply(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:90)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:90)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:243)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.dropDatabase(SessionCatalog.scala:221)
	at org.apache.spark.sql.execution.command.DropDatabaseCommand.run(ddl.scala:105)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)
	at retail.layers.stagingprocess$.stageprocess(stagingprocess.scala:18)
	at retail.driver.rundriver$.main(rundriver.scala:48)
	at retail.driver.rundriver.main(rundriver.scala)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	... 45 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	... 51 more
Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused (Connection refused)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:226)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:185)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:118)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:271)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:384)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:286)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:215)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:214)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:116)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:104)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.org$apache$spark$sql$hive$HiveSessionStateBuilder$$externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$1.apply(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$1.apply(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:90)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:90)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:243)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.dropDatabase(SessionCatalog.scala:221)
	at org.apache.spark.sql.execution.command.DropDatabaseCommand.run(ddl.scala:105)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)
	at retail.layers.stagingprocess$.stageprocess(stagingprocess.scala:18)
	at retail.driver.rundriver$.main(rundriver.scala:48)
	at retail.driver.rundriver.main(rundriver.scala)
Caused by: java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:221)
	... 59 more
)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:466)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	... 56 more
Failed to connect to the MetaStore Server...
Failed to connect to the MetaStore Server...
Failed to connect to the MetaStore Server...
Error Occured:java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;
======process started at 2022-04-19 7:47:44
Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.204.147 instead (on interface ens33)
Set SPARK_LOCAL_IP if you need to bind to another address
Running Spark version 2.4.7
Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Submitted application: Retail-coreengine
Changing view acls to: hduser
Changing modify acls to: hduser
Changing view acls groups to: 
Changing modify acls groups to: 
SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hduser); groups with view permissions: Set(); users  with modify permissions: Set(hduser); groups with modify permissions: Set()
Successfully started service 'sparkDriver' on port 33693.
Registering MapOutputTracker
Registering BlockManagerMaster
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
BlockManagerMasterEndpoint up
Created local directory at /tmp/blockmgr-e645ffba-8e8d-428e-bcd4-17e4091f1f89
MemoryStore started with capacity 1202.4 MB
Registering OutputCommitCoordinator
Logging initialized @10298ms
jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
Started @10613ms
Started ServerConnector@613a8ee1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
Successfully started service 'SparkUI' on port 4040.
Started o.s.j.s.ServletContextHandler@64ba3208{/jobs,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@13518f37{/jobs/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@7cbc3762{/jobs/job,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@49872d67{/jobs/job/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@56303b57{/stages,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4b2a01d4{/stages/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@8692d67{/stages/stage,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@34f7234e{/stages/stage/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@753432a2{/stages/pool,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@23bff419{/stages/pool/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4983159f{/storage,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@44e3a2b2{/storage/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@101639ae{/storage/rdd,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@4c550889{/storage/rdd/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@1d2bd371{/environment,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@44040454{/environment/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@65fe9e33{/executors,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@18bc345{/executors/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@42f8285e{/executors/threadDump,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@26bab2f1{/executors/threadDump/json,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@3724af13{/static,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@293a5f75{/,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@fcb4004{/api,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@784c3487{/jobs/job/kill,null,AVAILABLE,@Spark}
Started o.s.j.s.ServletContextHandler@53142455{/stages/stage/kill,null,AVAILABLE,@Spark}
Bound SparkUI to 0.0.0.0, and started at http://192.168.204.147:4040
Starting executor ID driver on host localhost
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34670.
Server created on 192.168.204.147:34670
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
Registering BlockManager BlockManagerId(driver, 192.168.204.147, 34670, None)
Registering block manager 192.168.204.147:34670 with 1202.4 MB RAM, BlockManagerId(driver, 192.168.204.147, 34670, None)
Registered BlockManager BlockManagerId(driver, 192.168.204.147, 34670, None)
Initialized BlockManager: BlockManagerId(driver, 192.168.204.147, 34670, None)
Started o.s.j.s.ServletContextHandler@64712be{/metrics/json,null,AVAILABLE,@Spark}
======staging process started at 2022-04-19 7:47:58
reading data from the file:s3a://inceptezspark/RetailData/Retail_Customers.csv
written data into hive table:retail_stg.tblcustomer_stg
reading data from the file:s3a://inceptezspark/RetailData/Retail_Product_Categories.csv
written data into hive table:retail_stg.tblproductcategory_stg
reading data from the file:s3a://inceptezspark/RetailData/Retail_Product_Subcategories.csv
written data into hive table:retail_stg.tblproductsubcategory_stg
reading data from the file:s3a://inceptezspark/RetailData/Retail_Sales*
The directory s3a://inceptezspark/RetailData/Retail_Sales* was not found. Was it deleted very recently?
written data into hive table:retail_stg.tblsales_stg
reading data from the file:s3a://inceptezspark/RetailData/Retail_Territories.csv
written data into hive table:retail_stg.tblterritory_stg
reading data from the file:s3a://inceptezspark/RetailData/Retail_Products.csv
written data into hive table:retail_stg.tblproduct_stg
======staging process completed at 2022-04-19 7:49:2
